\chapter{RELATED WORK}
\thispagestyle{plain}

\label{RelatedWork}

A number of different research projects have had similar motivation to the different compartments of \fw, but no other projects have attempted to build a comprehensive behavior space mapping system.
This chapter compares related work to \fw, and notes ways in which \fw has improved the state of the art in ABM behavior space modeling.
In Section \ref{sec:abmexp}, several projects in which researchers experimented with different configurations of ABMs are summarized.
Then, in Section \ref{sec:invfm}, different attempts at inverting the forward mapping are discussed.
Finally, Section \ref{sec:multilinear} gives an overview of Multilinear Interpolation, which was the inspiration for the simplical complex method for solving the reverse-mapping problem.

\section{Experimentation in ABMs}
\label{sec:abmexp}

There is a large body of experimental work using a simulation of a particular domain.
Researchers are typically either attempting to optimize a particular behavior or to match some desired system-level behavior.
Most of these approaches use exhaustive experimentation: a large number of configurations are sampled, and then the authors draw qualitative conclusions based on their findings.
These projects provide motivation for developing a forward mapping, because much of the analysis in this particular field is done manually, which is not necessary with a system such as \fw.
Also, more informed qualitative assessments could be made by researchers with the additional information provided by the functional mappings generated by \fw.

\subsection{A Platform for the Analysis of Artificial Self-organized Systems}
Perhaps the system most similar to \fw in motivation is an ABM experimental platform developed by Bourjot and Chevrier (2004)\nocite{bourjot2004platform}.
%Experimental platform for messing around with ABMs: (Bourjot -- ``A platform for the analysis of artificial self-organized systems'' 2004) (relevant?).
%Outline 3 tasks for experimentation -- under which conditions expected behavior occurs; validating the system (it works like we planned).
Their platform has the goal of being an automated and domain-independent system for determining under which conditions an expected system-level behavior occurs and for validating a system against true training data.
%designed a platform
%allows the user to experiment with a system with several inputs (analogous to our sampling).
The system provides sampling functionality for the user that finds the mean behavior of several agent-level configurations.
Once the data has been collected, plots can be generated that visualize how agent-level parameters affect the system-level properties.
The authors suggest that these data sets and plots can be a useful tool for making qualitative conclusions about the domain and the effect of changing agent-level parameters.
%they plot and find the mean behavior of different experiments
%they propose that you can use the raw data to compute abstract conclusions about the system.
%Suggest using optimization instead of a user.
%\fw provides a framework for general experimentation of ABMs.
%provides quantitative analysis, instead of qualitative. researchers' qualitative conclusions can be made with the assistance of the quantitative data produced by \fw.
\fw offers a superset of the features provided by the Bourjot and Chevrier platform.
The \fw sampling system automatically samples the behavior space, which can then be passed into a plotting program to generate visualizations.
In addition, \fw takes the approach of experimenting with different configurations of an ABM by learning the forward mapping to develop a functional representation of the space.

\subsection{Experimentation in Realistic Biological Models}
ABMs have been used to study behaviors in multi-agent models of biological systems such as ant colonies, locust swarms, and fish schools.
Researchers are typically interested in understanding how the agent-level policies that are followed by the individuals produce interesting system-level behavior.

Buhl et al. (2006)\nocite{buhl2006dom} studied a model of marching locusts in order to discover the critical density at which locusts start swarming, as opposed to acting independently.
At low population densities, locusts ignore their counterparts and graze for food as individuals.
Once a certain critical density of locusts is reached, they begin to swarm and collectively destroy crops.
Since experimenting with thousands of locusts is infeasible, the researchers utilized an agent-based model that simulates the behaviors of individual locusts.
The simulation exhibited the system-level threshold effect of swarming and not swarming, and the researchers were able to determine the value of the critical density through experimentation.
The authors note a few system-level properties that describe the swarming behavior, and presented plots that show the relationship between the agent-level configuration and these system-level properties.
This work is an example of manual experimentation, in contrast to \fw's automated approach.
Also, \fw would enable the researchers to specify a realistic critical density (i.e., one that has been observed in nature) and have the framework determine the set of configurations that would exhibit this critical density.
This tool would greatly enhance the ability for researchers to adjust their models to match true domain data.

Couzin et al. (2003)\nocite{couzin2003sol} used an agent-based model of a population of ants to study how multiple ``traffic lanes" form  from many ants leaving and entering a nest simultaneously.
%models how army ants form lanes.
%ants moving to and from the nest.
%at low densities, agents come and go randomly.
%at higher densities, lanes form that optimize traffic flow.
The authors found that at low ant densities, agents come and go randomly, without much concern for colliding with other ants.
At higher densities, lanes that optimize traffic flow form dynamically.
%agent-level property of avoidance turning rate and perception angle of area ahead of the ant.
In the experiments, the authors adjusted the avoidance turning rate (i.e., the angular velocity at which an ant is willing to turn to avoid another ant) and the perception angle ahead of the ant (i.e., the ant's field of view) in order to determine the effect of these parameters on the system-level lane formation behavior.
%the authors make a plot that show how ant flow changes with the change of these two parameters.
The values for the agent-level configurations were plotted against the traffic flow of the ants to demonstrate the correlation.
Turning rates and fields of view that were too low resulted in slow traffic flow, probably because the ants did not avoid one another enough, resulting in a number of collisions or ants waiting for other ants to pass.
Turning rates and fields of view that were too high resulted in slow traffic flow, as well, probably because the ants overcompensated to avoid other ants.
By plotting the data, researchers were able to determine the configuration of turning rate and field of view that would yield the optimal traffic flow.
%Also do another experiment where difference in turning rate between incoming and outgoing, shows that higher difference means higher flow.
In addition, the authors experimented with having different turning rates for incoming and outgoing agents different.
They found that if the outgoing agents had a higher turning rate than the incoming agents, the ants formed a three-lane system: incoming ants in the center and outgoing ants split, approaching the nest on the sides.
Intuitively, this property results because the incoming ants are less willing to avoid the outgoing ants, forcing the outgoing ants to the outside.
This lane formation is observed in army ants in the real world.
The authors found that increasing the difference between turning radii in outgoing and incoming ants generally increased the efficiency of traffic flow.
%They make plots and analyze them -- no computer-assistedness.
%Other parameters were tweaked to have the system match behavior from videos of real ants.
%This was probably done manually.
Most of the analysis that was done in this work was assisted by the plot visualizations of the behavior spaces.
In more complex domains with more dimensions, plotting may not be possible.
In cases such as these, models of the behavior space generated by \fw can be useful for making qualitative conclusions based on the data.

Parrish et al. (2002)\nocite{parrish2002sof} developed a generalized model of fish schooling that aggregates concepts from several schooling models.
%Generalized model of fish schooling that aggregate concepts from several previous schooling models.
In these models, a population of fish form into several subgroups or ``schools."
The authors emperically measured the effects of agent-level parameters on a number of school-level and population-level statistics, such as group size, number of groups, stragglers, collisions, and polarization.
Several different types of repulsion/attraction curves (the definition of how fish are attracted or repulsed by one another, based on distance) with different shapes and magnitudes were tested.
The authors draw a number of qualitative conclusions about the effects of agent-level parameters on the system-level properties.
They found that environmental factors such as drag and randomness had little effect on fish schooling behavior, in comparison to the social forces, such as avoidance.
With a convex attraction-repulsion curve, fish stayed closed together and had more collisions; however, the polarization (i.e., agreement on direction) was unaffected.
Changing the magnitude of neighbor scaling to have social forces be higher weighted based on distance made fish schools smaller and faster and decreased collisions.


\subsection{Experimentation in Particle Swarm Optimization}

Particle swarm optimization (PSO) is a swarm intelligence technique for finding a solution to optimization problems in a multi-dimensional, continuous search space \cite{kennedy1995pso}.
PSO uses a multitude of agents that ``swarm" around good solutions, hoping to find better solutions.
Agents in PSO are in predetermined neighborhoods in which all members of the same neighborhood share the neighborhood's highest fitness found.
Each agent keeps track of its personal highest fitness found, its neighborhood's highest fitness found and the global highest fitness found.
Then, an agent moves towards each of these maximum fitnesses with predetermined weights, specified by a parameter provided by the user.
Additional parameters specify the number of agents and how much momentum agents have (defined as how much of the agent's velocity vector in the previous time step is carried into the next time step).

PSO originally provided no guidelines for determining appropriate settings for the following parameters: the number of neighborhoods, the size of the neighborhoods, the constants in the force equation (personal best factor, neighborhood best factor, global best factor, momentum), the maximum speed of the agents, the initial spread of the agents, and the initial velocity.
One of the first attempts at developing a methodology for selecting parameter values was an empirical study of different configurations for particle swarm optimization \cite{shi1998parameter}.
The authors performed experiments to measure how the inertia weight (the momentum of a particle) and the maximum speed affect the performance of PSO.
The intuition behind this experiment is that if particles are too fast (i.e., have high inertia and high maximum speed), they will overshoot optimums, missing them entirely, while particles that are too slow (i.e., have low inertia and low maximum speed) will take a long time to reach optimal solutions.
A number of different configurations were used and the results are presented as a table.
There is no way to determine if these parameter values will transfer to target domains other than the experimental one.

A more general approach was attempted in a study by Van den Bergh and Engelbrecht (2006)\nocite{van2006study}, which provides a more theoretical perspective to particle swarm optimization than the previously discussed empirical research.
The focus of this work is to determine which configuration parameters will cause particles to converge on a good solution, as well as to predict the nature of the particle trajectories over time.
However, although this paper provides theoretical results for PSO, the authors explicitly state that the paper is not an approach to determining optimal parameters and points readers to empirical studies.
\fw can analyze systems such as PSO to find configurations that perform as expected, which takes the empirical studies one step further than just sampling data points by aggregating information from them.

\subsection{Models of Multi-Agent Systems}

The research summarized so far in this chapter used exhaustive experimentation to draw conclusions about the behavior space of a system.
In the two projects described in this section, meta-models (i.e., models about the model) of the system are defined and used to draw conclusions about the system.

Physicomimetics is a physics-based system for controlling multi-robot teams that was developed by Spears et al. (2004)\nocite{spears2004dpb}.
In this system, agents are considered objects that behave according to classic physics equations, such as $\vec{F} = m \vec{a}$.
Agents can perform a number of self-organizing tasks with these rules, such as forming hexagonal lattices and square formations.
The major benefit of using a physics-based control policy is that the system behavior can be analyzed using the equations, instead of having to experiment with the system.
System designers of this framework can avoid costly trial-and-error of the controllers by deriving theoretical laws from the controlling functions.
This benefit is similar in motivation to \fw: provide a way to interact with a multi-agent system with models, rather than experimentation.
Analysis of the system grounded in mathematics is useful and precise, but will not extend to domains other than the one it was designed for. 
Spears et al. specifically designed physicomimetics with the meta-model in mind, which is not possible for all ABMs; by contrast, the approach taken by \fw is applicable to many more domains.
This work was the main inspiration for using nonlinear regression as one of the forward-mapping methods.
Physicomimetics demonstrates that there are many benefits to having a concise mathematical model for a system: the ability to predict behavior by using equations and being able to prove concepts about the domain by interacting only with the model.
However, the main difference between this approach and \fw is that the physics models in physicomimetics were designed by human intuition,  while the models built by \fw are constructed using machine learning techniques and therefore domain-independent.

``Macroscopic models of swarm robot systems" \cite{lerman2002mmf,lerman2005rpm} are similar in motivation to \fw, but models the system in a more specific way (as finite state machines), and is therefore limited to systems in which agents can be modeled as FSAs.
My approach is more general, since it builds its models from system-level parameters of any type of ABM.
Lerman et al. (2002, 2005) propose a novel approach to modeling a multi-agent system comprised of finite state machine agents.
Since each robot is a finite state machine, this approach models agents as stochastic Markov processes (i.e., processes in which future state depends only on present state).
Next, several functions $N_x(t)$ are developed that predict the average number of agents in state $x$ at time $t$, which have a number of uses:
\begin{itemize}
\item Describe how the system will behave over time and whether or not it will eventually converge.
\item Summarize how much cumulative time has been spent in each state, which is useful for determining more abstract measurements of the system-level behavior. For example, the authors demonstrate the efficiency of a system based on how much cumulative time the agents spent avoiding one another, as opposed to foraging.
\item The length of time a task will take can be predicted. For example, if it is known that a total amount of $m$ minutes must be spent foraging to complete the task, one can determine the time step at which the integral of $N$ is $m$.
\end{itemize}
The authors used two domains: collaborative stick pulling and collaborative foraging.
They found that with more agents, the robots spend more time avoiding one another and less time collecting.
From this, the authors were able to determine how the efficiency of each robot is affected by group size, through experimentation.
The $N$ functions representat the system-level properties and are analogous in many ways to the \fw forward mapping.
However the approaches are fundamentally different because the functions are based on time $t$, not on the configuration of the system.
Also, requiring the agent behaviors to be describable as a Markov decision process limits the number of domains to which this research could be applied.
\fw is agnostic to the representation of the agent, which makes \fw more general.


\section{Inversion of Forward Mappings}
\label{sec:invfm}
The reverse-mapping problem has been tackled before by ``inverting" the forward mapping.
The idea is not new, but  \fw's approach of returning a functional approximation of the reverse space is novel.
All of these the previous approaches return a single solution, rather than a space, and many of these approaches are online.
This greatly reduces the usefulness of these approaches in making conclusions about the domain.

Learning the reverse mapping is typically an ill-posed problem because if the forward mapping is many-to-one, then the reverse mapping is a one-to-many relationship.
The major problem with directly learning this type of mapping is called the ``convexity problem" \cite{jordan-forward}: if the set of all possible solutions does not form a convex space, then the average of the inputs may land outside this space, and will therefore yield a poor result.
Many techniques, such as KNN and LOESS, depend on the average to generate accurate results.
For example,  approaches that simply directly learn the reverse mapping as a supervised learning problem, such as work by Widrow and Stearns (1985),\nocite{widrow1985adaptive} will not work in mappings that are not one-to-one.

Much work has been done in inverting multilayer neural networks and most of these approaches use some sort of optimization technique.
The goal is to determine which inputs to the neural network will generate a specified output.
These approaches do not return a mapping, which \fw does, but instead find a single solution that satisfies the desired system-level property.
Also, these techniques are tailored specifically for use with neural networks and are therefore not algorithm-independent.
Two such approaches are:
\begin{itemize}
\item Linden and Kindermann, ``Inversion of multilayer nets" (1989)\nocite{linden1989inversion} --
The authors use gradient descent to minimize $\sum (Y_i - f(X_i))^2$ by adjusting $X_i$.
That is, they incrementally adjust $X_i$ such that the difference between the desired output $Y_i$ and the actual output $f(X_i)$ is minimized.
\item Lu,  ``Inverting feedforward neural networks using linear and nonlinear programming'' (1990)\nocite{lu1999inverting} -- 
The author formulates the inverse problem as a linear programming problem and then uses a modified simplex method approach to solve it.
This approach has the benefit of being able to specify exterior constraints with linear programming.
Also, various points can be derived by setting different constraint functions, which allows the user to control the location of the output point.
With a gradient ascent approach, the region of the output point is unknown and unpredictable, which can be a problem in some situations.
\end{itemize}

Some more general approaches have been attempted for inverting the forward mapping in an algorithm-independent manner.
Lee and Kil (1989)\nocite{lee1994inverse}, present an iterative approach that can approximate any continuous function.
The algorithm incrementally approaches a better solution, and escapes local minima to continue searching for more solutions.
However, one requirement for running this approach is that the Jacobian must be calculated (or approximated), which makes this approach difficult to apply to certain functions, such as piecewise or discontinuous ones.
The authors suggest that their approach would work well with neural networks, since the Jacobian for a neural network is easy to compute.
As with the other approaches, this one returns one solution.

Stonedahl and Wilensky (2010)\nocite{stonedahl} use evolutionary search to look for non-convergence in the NetLogo Flocking and V-Formation Flocking (similar to flocking except that agents form into ``V" formations) domains.
Their approach uses genetic algorithms to search the configuration space for a particular system-level property of interest, or to optimize a particular behavior.
In particular, genetic algorithms are used to find the configuration that converges to a single heading as fast as possible.
This approach requires a quantitative measure of the system-level properties, similar to defining system-level properties in \fw.
An interesting addition to this approach is that it plots the results as a set of box-plots that show the ranges and distribution of configurations that generate desirable behaviors, which provides additional insight into the nature of the system-level behavior than optimization methods that return an individual point.
However, the correlations between the configuration parameters are mostly ignored in these plots.
That is, the ranges are shown, but how a single point in one parameter solution space maps to another one is not clear.
This approach is similar to ours in how it strives to build a representation of the reverse mapping; however, since the correlations between the system-level properties are not explicitely visualized, this mapping cannot be queried---it can only be used as an aid for qualitative analytics.
By contrast, \fw's reverse mapping can be queried to return a multi-dimensional solution space that describes all satisfiable configurations.


\section{Multilinear Interpolation}
\label{sec:multilinear}

  % Multilinear Interpolation
    % Concept - given corner points of a hypercube (knots), interpolate some point inside of it with linear interpolation; interpolate dimension my dimension until the point is reached.
Multilinear interpolation is a dynamic approach that uses multi-dimensional interpolation between ``knots" to generate a smooth surface across a space of any dimensionality \cite{davies1997multidimensional}.
Knots are sampled data points, scattered across the behavior space in a regular fashion such that the knots, when connected, form hypercubes.
When a point $\mathbf x$ is queried, multidimensional interpolation is performed using the corners of the hypercube to infer the value of $\hat y$.
A major downside to standard multilinear interpolation is that the sampling needs to be systematic and evenly spaced.
To remedy this situation, another regression algorithm can be used to compile a set of evenly spaced points.
\fw's simplical complex inversion uses this approach so that it can use randomly sampled data.
These evenly spaced inferred points are then passed to a traditional multilinear interpolation approach.

Multilinear interpolation serves as an inspiration for my SCI approach.
They are practically identical, except that SCI segregates the space into simplexes, instead of hypercubes (see Chapter \ref{ReverseMapping}).
This modification was necessary because intersections can be found at the edges of simplexes and connected with straight line segments.
Slices through a gradient in a hypercube that represent a particular value are not linear.
Therefore, the representation is more complicated and makes working with them more difficult.
SCI is therefore less smooth because of the linearity of the pieces, but easier to work with.

    % Downside - sampling needs to be systematic: remedy- use another regression algorithm to build the knots. This has the benefit of being faster than other approaches (the interpolation is fast, the regression may be slow, and the knots can be built ahead of time)

    % Faster than some of its counterparts
    % builds smooth mappings of multi-dimensional spaces




