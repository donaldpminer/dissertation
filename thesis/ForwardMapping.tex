\chapter{THE FORWARD-MAPPING PROBLEM}
\thispagestyle{plain}

\label{ForwardMapping}

% the forward mapping problem asks to develop a mapping from configuration space to system-level property space
The \textit{forward-mapping problem} is the problem of developing a mapping from an ABM's configuration space to the user-defined system-level property space.
The forward-mapping problem is the first half of building a complete meta-model of an agent-based model (the other half being the reverse-mapping problem).
Learning a meta-model is made simpler by splitting the problem into two sub-problems (as opposed to directly building a bidirectional mapping from the data).

% A user can use the forward mapping to predict the behavior of a system without having to run it.
The forward mapping can be used to predict the behavior of a system without having to actually run it.
% This could be useful for a number of reasons: it may be faster, it may be more convenient.
This is useful because running an ABM may require too much time for hypothetical experiments.
Also, in ABMs that have noisy behavior, several iterations of sampling may have to be performed to get an accurate average value.
This makes sampling the original ABM even more computationally intensive.
% Thus, this approach is more useful than interacting with the ABM directly to varify hypotheses about behavior, answering 'what-if' questions and manually exploring the behavior space faster.
Interacting with the forward mapping is more convenient than interacting with the ABM to verify hypotheses about system-level behavior.
Also, exploration of the behavior space is more efficient when interacting with a model than when running an ABM numerous times.


% This chapter details aspects of the forward mapping problem and the proposed solution used in \fw.
This chapter details aspects of the forward-mapping problem and the solution that is used in \fw.
% Also, evaluation criteria for solutions to the FMP is given.

\section{Definition of the Problem}
% develop an accurate mapping from //configuration space// to //system-level property space//.
The forward mapping is a functional mapping that maps \textit{configuration space} to \textit{system-level property space}.
The configuration space is made up of the dimensions representing all the input configuration parameters of the agent-based model that could affect the system-level behavior.
This space is $n$-dimensional, where $n$ is the number of configuration parameters.
For NetLogo ABMs, user interface elements such as sliders, text boxes, and switches provide user access to the parameters in the configuration space.

The system-level property space is made up of the dimensions representing all of the user-defined system-level properties that are being measured.
Typically, these properties are real-valued.
Since many system-level property metrics are statistical in nature, real values are used even when a discrete value is expected.
For example, an expected wolf population of 70.6 still has statistical meaning, even though it is impossible to have .6 of a wolf.
%The system-level property space is more open-ended that configuration space.
%Dimensions could be used to represent monitors or features of graphical plots, but, in general, system-level properties are user-defined statistical metrics.

The space defined by the combination of the configuration and the system-level property spaces is the \textit{behavior space}.

An outline of the configuration and system-level property spaces for the Wolf Sheep Predation ABM is shown in Table \ref{table:ws_parameters}.
In this domain, five configuration parameters and five system-level properties are considered; therefore the behavior space is ten-dimensional.


Let $\mathbf{y}$ be a point in system-level property space and $\mathbf x$ be a configuration.
The forward mapping $f$ is defined as the mapping that satisfies the following:
\[ f(\mathbf x) \rightarrow \mathbf{y} \]
% This entire mapping describes what is called the //behavior space//.
This mapping is used to answer the query ``given $\mathbf x$, approximate the system-level properties  $\hat{\mathbf y}$."
\[ f(\mathbf x) \approx \hat{\mathbf y} \]




\begin{table}[ht]
  \caption{Outline of Wolf Sheep Predation Behavior Space}
  \centering
  \begin{tabular}{c c c c}
    \hline \hline
    \textbf{Configuration Space} \\
    \hline
    grass-regrowth-time & Number of time steps it takes for grass to regrow \\
    sheep-gain-from-food & Energy gained by a sheep when it eats grass \\ 
    wolf-gain-from-food & Energy gained by a wolf when it eats a sheep \\
    sheep-reproduce & Probability that a sheep reproduces at each time step \\
    wolf-reproduce & Probability that a wolf reproduces at each time step\\
    \hline \hline
    \textbf{System-Level Property Space} \\
    \hline
    wolf-extinction & Probability that the wolves will go extinct \\
    wolf-population & Average wolf population, should it stabilize \\
    sheep-population & Average number of sheep \\
    wolf-variance & Variance of the wolf population \\
    sheep-variance & Variance of the sheep population \\
    \hline


  \end{tabular}
  \label{table:ws_parameters}
\end{table}



% A solution must be able to handle highly dimensional spaces and must be able to handle continuous and discontinuous configuration spaces.
A solution to the forward-mapping problem must be able to handle configuration spaces with many dimensions.
Also, the methods must be able to perform accurately in both continuous and discontinuous behavior spaces.
% Also, several evaluation criteria that could be used to analyze the effectiveness and efficiency of solutions are provided later in this chapter.
The most important aspect for any approach to solving the forward-mapping problem is that querying for a prediction must be faster than sampling the system directly and predictions should be as accurate as possible.

\section{The \fw Approach}

% The default approach taken by \fw to solve the forward-mapping problem is to do regression
Regression is the default approach taken by \fw to solve the forward-mapping problem because
% Regression fits this problem naturally, as regression takes indp variables and generates possible values for dep variables.
% The configuration parameters are the independent variables and the system-level properties are the dependent variables.
it fits this problem naturally: the configuration parameters are the independent variables and the system-level properties are the dependent variables.
% The sampling phase provides a data set with several instances of configuration, outcome observation pairs.
The sampling phase of \fw provides a data set with measurements from numerous and diverse configurations.
Each entry in the data set is a \textit{$<$configuration, system-level measurements$>$} pair.
% These are provided to the regression algorithm to base its predictions off of.
These individual observations are passed to the regression algorithm as training data.

% Different regression approaches use this data in different ways.
Different regression approaches use this data in different ways.
% Approaches like KNN do no pre-processing at all and uses the entire data set for each query.
Approaches like k-nearest neighbor (kNN) do not need to perform any pre-processing; instead they use the entire data set for each query.
% Meanwhile, approaches like Nonlinear Regression train a compact parametric model to represent the data and does not require the data set after this point.
Meanwhile, parametric approaches like nonlinear regression (NLR) train a compact model, using a limited number of parameters to represent the entire data set.
% In general, more time spent pre-processing equates into less time querying, but is not proportionate. 
In general, the more time that is spent pre-processing the data, the less time is spent querying.
Amortized over a large number of queries, offline computation to speed up online queries is a worthwhile trade-off.

% Other possible approaches fall into the field of machine learning.
%Other possible approaches typically fall into the field of machine learning.
% Approaches such as kernel methods and neural networks could solve the similar prediction problem here, but regression was selected particularly to make solving the reverse mapping problem, which is discussed in the next chapter.
%Approaches such as kernel methods and neural networks could solve the prediction problem.


\subsection{Scaling}

Calculating the distance between two observations is an important part of many nonparametric regression techniques.
Distance is used in kNN to determine which points will  be factored in to determine the output value, and in LOESS to weight closer points higher (hence, the ``locally-weighted" part of the name).
% Scaling the data set is an important factor in learning the forward mapping.
%Scaling of the data set to make is an important factor in learning the forward mapping.
% The problem is that the different configurations have different ranges and different meanings.

Different configuration parameters have different ranges and different meanings.
% Performing a simple euclidean distances on these is not sufficient, since the closeness of points will often be skewed on the values.
Therefore, performing a simple Euclidean distance on these parameters to determine distances between instances will typically not yield favorable results.
% For example, consider a percentage value in the WS domain (sheep birth rate, which may be sampled 1-8% or .01 to .08) and sheep-gain-from-food from 2-10.
For example, consider \textit{sheep-reproduce}, a percentage value in the Wolf Sheep Predation ABM, which is sampled from .01 to .08.
Another parameter, \textit{grass-regrowth}, is sampled from 5 to 30.
% The euclidean metric treats all dimensions as equal, so closeness in the larger ranged dimension will give higher weight than closeness in the smaller ranged dimension
The Euclidean distance metric treats all dimensions as equal, so closeness in the larger-ranged dimensions will be given lower weight than closeness in the smaller-ranged dimension.
%For example, values of .01 and .03 could be proportionally similar to 6 and 7, yet the Euclidean distances are very different: .02 and 1.
This skew in weight can make dimensions have less effect on the distance measurement, regardless of their actual importance.
Properly scaling the data set rectifies this problem.


% Typically, simply scaling each dimension such that the minimum value experienced is zero and the highest value is one, and then scaling all others accordingly works fine.
Simply scaling each dimension linearly such that the minimum and maximum values of each dimension are zero and one, respectively, is typically sufficient.
% However, more advanced techniques such as X, Y, Z have their advantages.
% A more in depth analysis on the effects of these approaches to the effectiveness of different approaches is left as future work (See \ref{sec:fw_scaling})
The analysis of the effect of more advanced sampling techniques is left as future work.


% The choice of scaling technique and distance metric is determined in the scope of the regression algorithm used, particularly in the training step.
% For example, KNN could preprocess the dataset into a scaled data set for future use.
Linear scaling is an optional step in \fw that can inserted between the sampling phase and the forward-mapping phase.
%Parameters for the minimum and maximum values are passed to the scale program, which  scales data points linearly, accordingly.
When \fw is queried, the points are rescaled to their original values.



\subsection{Handling Non-Continuous Configuration Spaces}

Ordinal (discrete) configuration parameters are handled as a special case in \fw.
%In designing \fw, I did not feel the need to handle discretely valued parameters as a typical use case since they are rather rare.
% The reason for the real-value assumption is just to simplify the process.
%Therefore, to streamline and simplify the processing of real-valued parameters, discrete valued parameters are handled differently in sampling and learning the forward mapping.
The sampling program is told whether each parameter is ordinal or real-valued.
The random selection of data points will abide by this restraint by randomly selecting integers for ordinal parameters, instead of real values

The best way to handle an ordinal parameter in learning the forward mapping depends on the nature of the variable.
If the parameter has a wide range, so that thousands of different values are possible, or if the number has little effect on the system, it may suffice to treat the parameter as a real value.
In many cases, the error incurred by making this assumption is negligible.
This method has the advantage of providing a common representation across all parameters.
Another approach would be to split the configuration space into separate configuration subspaces, with each subspace having a uniform value for the discrete parameter value.
This method provides more accurate results at the cost of higher computation requirements.
When there are numerous different discrete parameters, however, the number of subspaces grows quickly, making this approach intractable.


%Another problem caused by discrete values is the reverse mapping returns a real values for each of the parameters.
%The approaches used in \fw for the reverse mapping are not compatible with discrete values.
%The mappings will return impossible configurations as possible solutions to the reverse mapping problem.
%\fw leaves circumventing this problem to the configuration selection phase to select discretely valued configurations.
%This way, the reverse-mapping problem can be solved the same way for both real and discretely valued parameters.

\subsection{Handling Multi-Variate Forward Mappings}

% In the definition of the problem, I mention the fact that a number of system-level properties can be measured simultaneously.
Although multiple system-level properties can be measured simultaneously,
% To handle this, I split each system-level property into its individual single-property forward mapping problem.
\fw represents each system-level property as an independent single-property forward-mapping problem.
% give some math that splits the vector y into several individual ys
The reformulation of the original definition of the forward mapping is as follows:
\[ f_i(\mathbf x) \rightarrow y_i \]
where $i = \{0, 1, \ldots, m\}$, where $m = |\mathbf y|$.
To answer a  query for $\hat{\mathbf y}$, given $\mathbf x$, the sub-mappings are individually queried and collected:
\[ \{f_0(\mathbf x), f_1(\mathbf x), \ldots, f_{m}(\mathbf x)\} \rightarrow \hat{\mathbf y} \]
This approach assumes that the system-level properties are independent of one another.
If this assumption does not hold, the predictions may not be accurate.
The problem of correlated system-level properties is outside the scope of the implementation of \fw; in the test domains that have been used with \fw, the assumption appears to hold.



% The alternative to this would be to use a multi-variate regression approach, such as X.
% The main difference between these multi-variate approaches and my approach, is I have to assume that the dependent variables are statistically independent, i.e., they don't affect each other.
% This assumption may not always be the case, but I have not personally found this to be a problem in any of the ABMs I have experimented with.
% Also, the ability to use simple single-variate regression algorithms broadens the flexibility of the approach, makes it easier to implement and simplifies the solution to the reverse-mapping problem.
% An investigation for the effectiveness of multi-variate regression approaches for this research is left as outside the scope of this dissertation.

\subsection{Implementation Details}

% The solution to the forward mapping problem  is split into two distinct steps: training and predicting.
The \fw regression solution to the forward mapping is split into two distinct steps: training and predicting.
% training builds the model and/or preprocesses the data as necessary.
The training component builds the model and/or pre-processes the data as necessary.
% The predicting program allows the user or other components of the framework to query the regression model
The predicting component allows the user and other framework components (e.g., the reverse mapping) to query the regression model.
% This process is assisted by a user-provided regression library that is used to build the models.
This process is driven by a user-provided regression library that is plugged into the framework.
% More on how to build the module property to interface with \fw, see Appendix ??.
%More information on how to build this library in order to properly interface with \fw is provided in Appendix \ref{RegressionInterface}.

\subsubsection{Training}

% The training portion of solving the forward mapping is where any necessary necessary preprocessing and/or model building is done
During the training phase of solving the forward-mapping problem, any necessary pre-processing and/or model construction is performed.
% Also, scaling of the data can be done in this step.
% This is implemented as a Python program called train.py. train.py takes in the data set generated by the sampling as well as a user-created regression module that was built to interface with train.py.
This process is implemented as a Python function called \textit{train}.
%\textit{train} takes the training data set as input and uses this data
%to prepare and set up the specific regression approach passed in by the user.
% train.py then passes the data set to the regression module, which then may produce model files on the local filesystem.
Most regression algorithms will store models or meta-data in memory for later use.
% KNN may simply just scale the data, or with more advanced implementations build a geo hash or kd-tree.
Basic kNN computes nothing since the queries use the entire data set.
% LOESS will perform the locally weighted smoothing step to build $y'$ values. (and output the smoothed data set)
LOESS performs the iterative locally weighted smoothing step to build $y'$ values, which are
written to a file.
% NLR will learn optimal parameters for the parametric model given with optimization. (and output the parameters)
Nonlinear regression learns the optimal parameters for the specified parametric model by using optimization; the parameters for this model are stored.
% MLI will build a evenl spread data set with another regression algorithm
%Multi-linear interpolation approaches use this phase to develop an evenly spaced data set with another regression approach.
% Each of these approaches require varying computational time
Each of these approaches requires varying amounts of computational time, but needs to be executed only once for a given data set.


\subsubsection{Predicting}

% The prediction step is the step in which queries are passed and predictions are returned.
The prediction phase of solving the forward mapping problem  is invoked whenever a query is submitted.
% This is implemented as a python program predict.py that takes the regression module and any trained meta-data to generate predictions (answers to user-specified queries)
This process is implemented as a Python program called \textit{predict}, but is mostly driven by the user-provided regression module.
The \textit{predict} function in \fw uses the models generated by \textit{train} to predict values for individual system-level properties.
% Any number of queries in the form of a configuration vector are passed in through standard in, with answered returned in standard out.

As in the training phase, the predicting phase is algorithm-dependent; however, the API users interact with is the same.
% KNN will iterate through every point to find the kNN, then average the values
kNN will iterate over each input point to find the nearest neighbors, then average the values.
% LOESS will interpolate between nearby smoothed data set points to determine the new value
LOESS will interpolate between nearby smoothed training data points.
% NLR will simply plug in values for the configuration parameters and calculate the result
NLR applies the parametric model using the input configuration values.
% MLI will find which bin the point lies within and then interpolate from the corners.
Each of these approaches requires varying amounts of computational time, as with the training step.
Since this process is executed once per  query, efficiency is an important factor.


\section{Using Forward Mapping to Solve the Reverse-Mapping Problem}

% The forward mapping model has two main uses: prediction of behavior for a user and used by \fw for the reverse mapping.
The forward mapping has two main uses: it is used to predict the behavior of a system, given the configuration; and it is used by \fw to solve the reverse mapping problem.
% \fw uses the forward mapping to solve the reverse mapping.
%\fw uses the forward mapping to solve the reverse mapping.
% A number of approaches are discussed in Chapter \ref{ReverseMapping}: The Reverse-Mapping Problem, but practically all use the forward mapping instead of direct sampling for the same reasons the user uses this over it.
\fw uses the forward mapping to solve the reverse-mapping problem.
All of these approaches use the forward mapping instead of directly sampling from the ABM for two reasons.
First, querying models of the behavior space is faster than interacting with the ABM directly.
Second, the regression models smooth the error over the entire space, effectively eliminating natural sampling noise, which can produce more accurate and consistent results.
% The forward mapping describes behavior faster than direct sampling, and allows faster searching of the space to answer queries.



\section{Summary}

The forward-mapping problem is the problem of developing a mapping that predicts system-level behavior, given the agent-level configuration parameter values.
The solution to this problem has a central role in the framework, as it provides the ability for users to predict behavior and is used to solve the reverse-mapping problem.

My solution to the forward-mapping problem is to use regression to learn the correlations between the configuration parameters (independent variables) and the system-level properties (dependent variables), individually.
Different regression algorithms, in the form of a Python module library, can be plugged into \fw seamlessly.
This allows for a great amount of flexibility in approaches that can be used to sample ABMs.
\fw adapts with new technology as new regression techniques are developed, since they can simply be plugged in by the user.








